#盒鬚圖（箱型）：根據max、min、Med、四分衛數Q1+Q3
library(C50)
data(churn)
attach(churnTrain)
boxplot(total_day_minutes, horizontal=TRUE, xlab="白天通話分鐘數", col="pink")
#顯示白天、晚上與半夜的通話分鐘數
dt <- data.frame(total_eve_minutes, total_night_minutes, total_day_minutes)
boxplot(dt, horizontal=TRUE, xlab="通話分鐘數", col="pink")
#顯示白天、晚上與半夜的通話分鐘數
dt <- data.frame(total_eve_minutes, total_night_minutes, total_day_minutes)
boxplot(dt, horizontal=TRUE, xlab="通話分鐘數", col="pink")
#顯示白天、晚上與半夜的通話分鐘數
dt <- data.frame(total_eve_minutes, total_night_minutes, total_day_minutes)
boxplot(dt, horizontal=TRUE, xlab="通話分鐘數", col="pink")
#盒鬚圖（箱型）：根據max、min、Med、四分衛數Q1+Q3
library(C50)
data(churn)
attach(churnTrain)
boxplot(total_day_minutes, horizontal=TRUE, xlab="白天通話分鐘數", col="pink")
#顯示白天、晚上與半夜的通話分鐘數
dt <- data.frame(total_eve_minutes, total_night_minutes, total_day_minutes)
boxplot(dt, horizontal=TRUE, xlab="通話分鐘數", col="pink")
# 顯示白天、晚上與半夜的通話分鐘數horizontal=F直圖
boxplot(dt, horizontal=FALSE, xlab="通話分鐘數", col=terrain.colors(3))
legend("topright", title="撥打分鐘數", c('eve','night','day'), fill=terrain.colors(3), horiz=FALSE, ncol = 1, cex = 0.6)
# 顯示白天、晚上與半夜的通話分鐘數horizontal=F直圖
boxplot(dt, horizontal=FALSE, xlab="通話分鐘數", col=terrain.colors(3))
legend("topright", title="撥打分鐘數", c('eve','night','day'), fill=terrain.colors(3), horiz=FALSE, ncol = 1, cex = 0.6)
#分組的盒鬚圖，檢視在不同地區流失客戶與未流失客戶於晚上 通話時間的分布
boxplot(total_eve_minutes~area_code*churn,horizontal=FALSE, xlab="夜晚通話分鐘數",col=terrain.colors(3))
#分組的盒鬚圖，檢視在不同地區流失客戶與未流失客戶於晚上 通話時間的分布
boxplot(total_eve_minutes~area_code*churn,horizontal=FALSE, xlab="夜晚通話分鐘數",col=terrain.colors(3))
# 簡單圓餅圖
pieces <- c(8,8,2,4,2) > pie(pieces , labels = c('工作','睡覺','念書','打電動','聊天'), main="生活時 間分配圖") 課
# 簡單圓餅圖
pieces <- c(8,8,2,4,2) > pie(pieces , labels = c('工作','睡覺','念書','打電動','聊天'), main="生活時 間分配圖") 課
# 簡單圓餅圖
pieces <- c(8,8,2,4,2) > pie(pieces , labels = c('工作','睡覺','念書','打電動','聊天'), main="生活時 間分配圖") 課
# 簡單圓餅圖
pieces <- c(8,8,2,4,2)
pie(pieces , labels = c('工作','睡覺','念書','打電動','聊天'), main="生活時 間分配圖") 課
pie(pieces , labels = c('工作','睡覺','念書','打電動','聊天'), main="生活時 間分配圖")
pieces <- c(8,8,2,4,2)
#簡單圓餅圖 加上百分比
pieces <- c(8,8,2,4,2)
pct <- round(pieces/sum(pieces)*100)#計算各個類別百分比
lbls <- paste(c('工作','睡覺','念書','打電動','聊天'),pct,'%', sep='')
pie(pieces , labels = lbls, main="生活時間分配圖")
#簡單圓餅圖 加上百分比
pieces <- c(8,8,2,4,2)
pct <- round(pieces/sum(pieces)*100)#計算各個類別百分比
lbls <- paste(c('工作','睡覺','念書','打電動','聊天'),pct,'%', sep='')
pie(pieces , labels = lbls, main="生活時間分配圖")
pie(pieces , labels = lbls, main="生活時間分配圖")
# 利用線圖看趨勢
years <- sort(round(runif(10,2000,2010),0))
newbornbaby<- sort(round(runif(10,200,1000),0))
dt <- data.frame(newbornbaby,years)
par(mfrow=c(3,2))#在一張畫布上輸出3*2個圖型
plot(newbornbaby~ years , data=dt, type='l', col=1) #只畫線
plot(newbornbaby~ years , data=dt, type='b', col=2) #畫線與點
plot(newbornbaby~ years , data=dt, type='c', col=3) #把'b'
plot(newbornbaby~ years , data=dt, type='h', col=4) #垂直線
plot(newbornbaby~ years , data=dt, type='s', col=5) #階梯圖
packages<-c("C50","tree","rpart","randomForest")
for (i in packages){install.packages(i)}
#一次載入
sapply((packages, FUN=library,character.only=T)
#一次載入
sapply(packages, FUN=library,character.only=T)
search()
install.packages("caret")
library(caret)
sample_index<-createDataPartition(y=iris$Species,p=0.7,list=F)
iris.train=iris[smaple_index,]
sample_index<-createDataPartition(y=iris$Species,p=0.7,list=F)
iris.train=iris[smaple_index,]
iris.test=iris[-smaple_index,]
iris.train=iris[sample_index,]
iris.test=iris[-sample_index,]
#確認訓練樣本與測試樣本不一致
par(mfrow=c(1,2))
plot(iris.train$Species)
plot(iris.test$Species)
#模型訓練
iris.C50tree=C5.0(Species~.,data=iris.train)
summary(iris.C50tree)
plot(iris.C50tree)
#模型訓練
iris.C50tree=C5.0(Species~.,data=iris.train) #"~."取所有X變數
summary(iris.C50tree)
plot(iris.C50tree)
#訓練樣本的confusion matrix與預測準確率
y=iris$Species[smaple_index]
#訓練樣本的confusion matrix與預測準確率
y=iris$Species[sample_index]
y_hat=predict(iris.C50tree,iris.train,type='class')
table.train=table(y,y_hat)
cat("Total records(train)=",nrow(iris.train),"\n")
#預測正確率= 矩陣對角對角總和/矩陣總和
cat("Correct Classification Ratio(train)=", sum(diag(table.train))/sum(table.train)*100,"%\n")
#測試樣本的confusion matrix與預測準確率
y = iris$Species[-sample_Index]
#測試樣本的confusion matrix與預測準確率
y = iris$Species[-sample_index]
y_hat= predict(iris.C50tree,iris.test,type='class')
table.test=table(y,y_hat)
cat("Total records(test)=",nrow(iris.test),"\n")
cat("Correct Classification Ratio(test)=", sum(diag(table.test))/sum(table.test)*100,"%\n")
#測試樣本的confusion matrix與預測準確率
y = iris$Species[-sample_index]
y_hat= predict(iris.C50tree,iris.test,type='class')
table.test=table(y,y_hat)
cat("Total records(test)=",nrow(iris.test),"\n")
cat("Correct Classification Ratio(test)=", sum(diag(table.test))/sum(table.test)*100,"%\n")
#確認訓練樣本與測試樣本不一致
par(mfrow=c(1,2))
plot(iris.train$Species)
plot(iris.test$Species)
#模型訓練
iris.C50tree=C5.0(Species~.,data=iris.train) #"~."取所有X變數
summary(iris.C50tree) #產生混淆矩陣或得用Length及Width即可判別品種
plot(iris.C50tree)
#訓練樣本的confusion matrix與預測準確率
y=iris$Species[sample_index]
y_hat=predict(iris.C50tree,iris.train,type='class')
table.train=table(y,y_hat)
cat("Total records(train)=",nrow(iris.train),"\n")
#預測正確率= 矩陣對角對角總和/矩陣總和
cat("Correct Classification Ratio(train)=", sum(diag(table.train))/sum(table.train)*100,"%\n")
#測試樣本的confusion matrix與預測準確率
y = iris$Species[-sample_index]
y_hat= predict(iris.C50tree,iris.test,type='class')
table.test=table(y,y_hat)
cat("Total records(test)=",nrow(iris.test),"\n")
cat("Correct Classification Ratio(test)=", sum(diag(table.test))/sum(table.test)*100,"%\n")
#確認訓練樣本與測試樣本不一致
par(mfrow=c(1,2))
plot(iris.train$Species)
plot(iris.test$Species)
#模型訓練
iris.C50tree=C5.0(Species~.,data=iris.train) #"~."取所有X變數
summary(iris.C50tree) #產生混淆矩陣或得用Length及Width即可判別品種
plot(iris.C50tree)
plot(iris.C50tree)
#訓練樣本的confusion matrix與預測準確率
y=iris$Species[sample_index]
y_hat=predict(iris.C50tree,iris.train,type='class')
table.train=table(y,y_hat)
cat("Total records(train)=",nrow(iris.train),"\n")
#預測正確率= 矩陣對角對角總和/矩陣總和
cat("Correct Classification Ratio(train)=", sum(diag(table.train))/sum(table.train)*100,"%\n")
#測試樣本的confusion matrix與預測準確率
y = iris$Species[-sample_index]
y_hat= predict(iris.C50tree,iris.test,type='class')
table.test=table(y,y_hat)
cat("Total records(test)=",nrow(iris.test),"\n")
cat("Correct Classification Ratio(test)=", sum(diag(table.test))/sum(table.test)*100,"%\n")
cat("Correct Classification Ratio(test)=", sum(diag(table.test))/sum(table.test)*100,"%\n")
iris.RFtree=randomForest(Species~.,data=iris.train,importance=T,proximity=T,ntree=300)
print(iris.RFtree)
data()
data(churn)
?(churn)
churn.tree=rpart(churn~.,data = data_train)
#模型訓練
data_train=churnTrain[,-3] #排除"area_code"欄位
churn.tree=rpart(churn~.,data = data_train)
churn.tree
names(churnTrain)
str(churnTrain)
#CART決策樹
plot(churn.tree)
#CART決策樹
plot(churn.tree)
text(chrun,tree=.8)
#CART決策樹
plot(churn.tree)
text(chrun,tree=.8)
text(chrun.tree,cex=.8)
#CART決策樹
plot(churn.tree)
text(chrun.tree,cex=.8)
#模型訓練
data_train=churnTrain[,-3] #排除"area_code"欄位
churn.tree=rpart(churn~.,data = data_train)
churn.tree
#CART決策樹
plot(churn.tree)
text(chrun.tree,cex=.8)
#CART決策樹
plot(churn.tree)
text(chrun.tree,cex=.8)
text(churn.tree,cex=.8)
#CART決策樹
plot(churn.tree)
text(churn.tree,cex=.8)
#變數重要性
churn.tree$variable.importance
#訓練樣本的confusion matrix與預測準確率
y=churnTrain$churn
y_hat=predict(churn.tree,newdata = churnTrain,type='class')
table.train=table(y,y_hat)
cat("Total records(train)=",nrow(iris.train),"\n")
#預測正確率= 矩陣對角對角總和/矩陣總和
cat("Correct Classification Ratio(train)=", sum(diag(table.train))/sum(table.train)*100,"%\n")
#測試樣本的confusion matrix與預測準確率
y = churnTrain$churn
y_hat= predict(churn.tree,newdata = churnTest,type='class')
table.test=table(y,y_hat)
y_hat= predict(churn.tree,newdata = churnTest,type='class')
table.test=table(y,y_hat)
#測試樣本的confusion matrix與預測準確率
y = churnTrain$churn
y_hat= predict(churn.tree,newdata = churnTest,type='class')
table.test=table(y,y_hat)
#測試樣本的confusion matrix與預測準確率
y = churnTest$churn
y_hat= predict(churn.tree,newdata = churnTest,type='class')
table.test=table(y,y_hat)
cat("Total records(test)=",nrow(iris.test),"\n")
cat("Correct Classification Ratio(test)=", sum(diag(table.test))/sum(table.test)*100,"%\n")
setwd(E:/DM/work)
setwd("E:/DM/work")
#模型訓練
data_train=churnTrain #排除"area_code"欄位
churn.tree=rpart(churn~.,data = data_train)
churn.tree
#模型訓練
data_train=churnTrain[,-3] #排除"area_code"欄位
churn.tree=rpart(churn~.,data = data_train)
churn.tree
setwd("E:/DM/work")
names(churnTrain)
str(churnTrain)
source('Gain_lift_Chart.r',encoding = 'Big5')
names(churnTrain)
str(churnTrain)
#模型訓練
data_train=churnTrain #排除"area_code"欄位
churn.tree=rpart(churn~.,data = data_train)
churn.tree
#CART決策樹
plot(churn.tree)
text(churn.tree,cex=.8)
#模型訓練
data_train=churnTrain[,-3] #排除"area_code"欄位
churn.tree=rpart(churn~.,data = data_train)
churn.tree
#CART決策樹
plot(churn.tree)
text(churn.tree,cex=.8)
#模型訓練
data_train=churnTrain[,-3] #排除"area_code"欄位
churn.tree=rpart(churn~.,data = data_train)
churn.tree
#CART決策樹
plot(churn.tree)
text(churn.tree,cex=.8)
#變數重要性
churn.tree$variable.importance
#訓練樣本的confusion matrix與預測準確率
y = churnTrain$churn
y_hat=predict(churn.tree,newdata = churnTrain,type='class')
y_prob=predict(churn.tree,newdata = churnTest,type="prob")[,1] #取正確預測機率
#ROC
install.packages("ROCR")
library(ROCR)
pred<-prediction(y_prob,labels = churnTest$churn)
# tpr: True Positive Ratio 正確預測正例
# fpr: False Positive Ration誤判為正例
perf <- performance(pred, "tpr", "fpr")
plot(perf)
plot(perf)
points(c(0,1),c(0,1),type="l",lty=2)  #畫虛線
#AUC
perf <- performance(pred, "auc")
( AUC = perf@y.values[[1]] )
( Gini = (AUC-0.5) *2 )*100
# Iift chart
perf <- performance(pred, "lift" , "rpp")
plot(perf)
source('Gain_lift_Chart.r',encoding = 'Big5')
#載入C50 churn資料集
data(churn) #載入C50 churn資料集
data_train = churnTrain[,-3] #排除 "area_code"欄位
#載入C50 churn資料集
data(churn) #載入C50 churn資料集
data_train = churnTrain[,-3] #排除 "area_code"欄位
data_train = churnTrain[,-1] # 排除 “state"欄位
data_train$churn = ifelse(data_train$churn=='yes',1,0)  # 羅吉斯回歸預設對 y=1 建模產出推估機率
#模型訓
logitM=glm(formula=churn~., data= data_train, family=binomial(link="logit"), na.action=na.exclude)
summary(logitM)
source('Gain_lift_Chart.r', encoding = 'Big5') #老師寫的UDF
y = ifelse(churnTest$churn=='yes',1,0) #記得要把正例轉成1，負例轉成0
y_hat=predict(churn.tree,newdata=churnTest,type="class")
y_prob = predict(churn.tree,newdata=churnTest,type="prob") #預測流失機率
#呼叫老師寫的UDF
DT =Gain_Lift_Chart(y,y_hat,y_prob)
par(mfrow = c(1, 2))
# Gain Chart
plot(DT$row_index, DT$target_rto, xlab = "全體人數累積百分比",  ylab = "正例人數累積百分比" ,type = "l", main = "Gain Chart")
#Lift Chart
plot(DT$row_index, DT$lift, xlab = "全體人數累積百分比", ylab = "Lift",type = "l", main = "Lift Chart")
# Gain Chart
plot(DT$row_index, DT$target_rto, xlab = "全體人數累積百分比",  ylab = "正例人數累積百分比" ,type = "l", main = "Gain Chart")
#Lift Chart
plot(DT$row_index, DT$lift, xlab = "全體人數累積百分比", ylab = "Lift",type = "l", main = "Lift Chart")
#訓練樣本的混淆矩陣(confusion matrix)與預測正確率
install.packages("InformationValue") # for optimalCutoff
library(InformationValue)
y = data_train$churn
y_hat=predict(logitM,newdata=churnTrain,type="response")
#optimalCutoff(y, y_hat)[1] 提供了找到最佳截止值，減少錯誤分類錯誤
table.train=table(y, y_hat > optimalCutoff(y, y_hat)[1] )
#預測正確率 = 矩陣對角對角總和/矩陣總和
cat("Correct Classification Ratio(train)=", sum(diag(table.train))/sum(table.train)*100,"%\n")
#測試樣本的混淆矩陣(confusion matrix)與預測正確率
y = ifelse(churnTest$churn=='yes',1,0)
y_hat=predict(logitM,newdata=churnTest,type="response")
table.test=table(y, y_hat > optimalCutoff(y, y_hat)[1] )
#預測正確率 = 矩陣對角對角總和/矩陣總和
cat("Correct Classification Ratio(test)=", sum(diag(table.test))/sum(table.test)*100,"%\n")
# ROC Curve
library(ROCR)
y_prob <- predict(logitM,newdata=churnTest,type="response")
y_prob <- exp(y_prob)/(1+exp(y_prob)) #odds轉機率
pred <- prediction(y_prob, labels = churnTest$churn)
# tpr: True Positive Ratio正確預測正例
# fpr: False Positive Ration誤判為正例
perf <- performance(pred, "tpr", "fpr")
plot(perf)
points(c(0,1),c(0,1),type="l",lty=2)  #畫虛線
#AUC
perf <- performance(pred, "auc")
( AUC = perf@y.values[[1]] )
#Gini
( Gini = (AUC-0.5) *2 )*100
# tpr: True Positive Ratio正確預測正例
# fpr: False Positive Ration誤判為正例
perf <- performance(pred, "tpr", "fpr")
plot(perf)
points(c(0,1),c(0,1),type="l",lty=2)  #畫虛線
#AUC
perf <- performance(pred, "auc")
( AUC = perf@y.values[[1]] )
#Gini
( Gini = (AUC-0.5) *2 )*100
install.packages("neuralnet") #多層神經網路:倒傳遞類神經網路
install.packages("nnet") #單層神經網路
library(nnet)
install.packages("nnet")
library(neuralnet)
library(nnet)
library(neuralnet)
data(iris)
# One-hot Encoding
head(class.ind(iris$Species))
data <- cbind(iris, class.ind(iris$Species))
# 產生建模與測試樣本7:3
n=0.3*nrow(data)
test.index=sample(1:nrow(data),n)
Train=data[-test.index,]
Test=data[test.index,]
# 建模
formula.bpn <- setosa + versicolor + virginica ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width
BNP = neuralnet(formula = formula.bpn,
hidden=c(3,2), # 兩層隱藏層，第一層有3個神經元，第二層有2個神經元
data=Train,
learningrate = 0.01, # learning rate
threshold = 0.01,
stepmax = 5e5 # 最大的ieration數 = 500000(5*10^5)
)
# 繪製網路圖
plot(BNP)
# 預測
cf=compute(BNP,Test[,1:4])
Ypred = as.data.frame(round(cf$net.result)) #預測結果
# 建立一個新欄位，叫做Species
Ypred$Species <- ""
# 把預測結果轉回Species的型態
for(i in 1:nrow(Ypred)){
if(Ypred[i, 1]==1){ Ypred[i, "Species"] <- "setosa"}
if(Ypred[i, 2]==1){ Ypred[i, "Species"] <- "versicolor"}
if(Ypred[i, 3]==1){ Ypred[i, "Species"] <- "virginica"}
}
Ypred
table(Test$Species,Ypred$Species)
#  混淆矩陣 (預測率有95.56%)
message("準確度：",sum(diag(table(Test$Species,Ypred$Species))) / sum(table(Test$Species,Ypred$Species)) *100,"%")
#  混淆矩陣 (預測率有95.56%)
message("準確度：",sum(diag(table(Test$Species,Ypred$Species))) / sum(table(Test$Species,Ypred$Species)) *100,"%")
# 混淆矩陣 (預測率有97.7777%)
message("準確度：",sum(diag(table(Test$Species,Ypred$Species))) / sum(table(Test$Species,Ypred$Species)) *100,"%")
# 建模
formula.bpn <- setosa + versicolor + virginica ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width
BNP = neuralnet(formula = formula.bpn,
hidden=c(2,4,8), #兩層隱藏層，第一層有3個神經元，第二層有2個神經元
data=Train,
learningrate = 0.01, # learning rate
threshold = 0.01, #自設參數勿太大->收斂
stepmax = 5e5 #最大的ieration數 = 500000(5*10^5)
)
# 繪製網路圖
plot(BNP)
# 預測
cf=compute(BNP,Test[,1:4])
# 繪製網路圖
plot(BNP)
BNP = neuralnet(formula = formula.bpn,
hidden=c(2,4,8,10), #兩層隱藏層，第一層有3個神經元，第二層有2個神經元
data=Train,
learningrate = 0.01, # learning rate
threshold = 0.01, #自設參數勿太大->收斂
stepmax = 5e5 #最大的ieration數 = 500000(5*10^5)
)
# 繪製網路圖
plot(BNP)
# 預測
cf=compute(BNP,Test[,1:4])
iris_new<-iris[.-5]
iris_new<-iris[.-5]
Cluster_km<-kmeans(iris_new,nstart = 15,centers = 3) #nstart隨機丟太集中最多重算次數；centers群數
#nstart是指重新隨意放k個中心點的次數，一般建議nstart>=10
table(Cluster_km$cluster, iris[, 5]) #觀察分群結果與實際類別的差別
iris_new <- iris[.-5]
iris_new <- iris[,-5]
Cluster_km<-kmeans(iris_new,nstart = 15,centers = 3) #nstart隨機丟太集中最多重算次數；centers群數
#nstart是指重新隨意放k個中心點的次數，一般建議nstart>=10
table(Cluster_km$cluster, iris[, 5]) #觀察分群結果與實際類別的差別
plot(iris_new$Petal.Width,iris_new$Petal.Length,col=Cluster_km$cluster)
plot(iris_new$Petal.Width,iris_new$Petal.Length,col=Cluster_km$cluster)
Cluster_km<-kmeans(iris_new,nstart = 15,centers = 3) #nstart隨機丟太集中最多重算次數；centers群數
#nstart是指重新隨意放k個中心點的次數，一般建議nstart>=10
table(Cluster_km$cluster, iris[, 5]) #觀察分群結果與實際類別的差別
plot(iris_new$Petal.Width,iris_new$Petal.Length,col=Cluster_km$cluster)
Cluster_km
plot(iris_new$Petal.Width,iris_new$Petal.Length,col=Cluster_km$cluster)
iris_new <- iris[,-5]
Cluster_km<-kmeans(iris_new,nstart = 15,centers = 3) #nstart隨機丟太集中最多重算次數；centers群數
#nstart是指重新隨意放k個中心點的次數，一般建議nstart>=10
table(Cluster_km$cluster, iris[, 5]) #觀察分群結果與實際類別的差別
plot(iris_new$Petal.Width,iris_new$Petal.Length,col=Cluster_km$cluster)
Cluster_km
WSS_ratio<-rep(NA,times=10)
#最佳分群數K決定
Cluster_km
WSS_ratio<-rep(NA,times=10)
for (k in 1:length(WSS_ratio)) {
Cluster_km<- kmeans(iris_new, nstart=15,centers=k)
WSS_ratio[k] <- Cluster_km$tot.withinss
}
#畫陡坡圖
plot(WSS_ratio, type="b", main = "陡坡圖")
setwd('E:/DM/work/Unit4')
x=read.table("三國武將資料集.csv", header=T, sep=",") #讀取武將資料集
model_data <- data.frame(x$武將, x$統率, x$武力, x$智力, x$政治) #讀取切割變數
set.seed(123)#設定隨機種子
WSS_ratio <- rep(NA, times = 10) #設定組內距離平方和變數
for (k in 1:length(WSS_ratio)) #建置
#畫陡坡圖
plot(WSS_ratio, type="b", main = "陡坡圖")
Cluster_km <- kmeans(model_data[-1], nstart=15,centers=3) #建模，群數K=3
#安裝資料清理工作包
install.packages("dplyr")
library(dplyr)
with_model_data <- tbl_df( final_data ) #轉成dplyr格式
result <with_model_data %>% dplyr::group_by(cluster) %>%
write.table(result , file='result.csv', col.names=T, row.names=F, sep=",", quote = F) #結果寫出成csv檔
library(dplyr)
with_model_data <- tbl_df( final_data ) #轉成dplyr格式
result <with_model_data %>% dplyr::group_by(cluster) %>%
summarise(
count = n(),
median_統率 = median(統率, na.rm = TRUE),
median_武力 = median(武力, na.rm = TRUE),
median_智力 = median(智力, na.rm = TRUE),
median_政治 = median(政治, na.rm = TRUE)
) #分析各群切割變數
library(dplyr)
with_model_data <- tbl_df( final_data ) #轉成dplyr格式
result <-
with_model_data %>%
dplyr::group_by(cluster) %>%
summarise(
count = n(),
median_統率 = median(統率, na.rm = TRUE),
median_武力 = median(武力, na.rm = TRUE),
median_智力 = median(智力, na.rm = TRUE),
median_政治 = median(政治, na.rm = TRUE)
) #分析各群切割變數
write.table(result , file='result.csv', col.names=T, row.names=F, sep=",", quote = F) #結果寫出成csv檔
library(dplyr)
with_model_data <- tbl_df( final_data ) #轉成dplyr格式
Cluster_km <- kmeans(model_data[-1], nstart=15,centers=3) #建模，群數K=3
final_data <- data.frame(x,cluster=as.character(Cluster_km$cluster)) #將原始資料集與模型結果進行比對
with_model_data <- tbl_df( final_data ) #轉成dplyr格式
result <-
with_model_data %>%
dplyr::group_by(cluster) %>%
summarise(
count = n(),
median_統率 = median(統率, na.rm = TRUE),
median_武力 = median(武力, na.rm = TRUE),
median_智力 = median(智力, na.rm = TRUE),
median_政治 = median(政治, na.rm = TRUE)
) #分析各群切割變數
write.table(result , file='result.csv', col.names=T, row.names=F, sep=",", quote = F) #結果寫出成csv檔
subset(final_data,  final_data$cluster==1)[,1:5]  #查看群一武將
subset(final_data,  final_data$cluster==2)[,1:5]  #查看群二武將
subset(final_data,  final_data$cluster==3)[,1:5]  #查看群三武將
